% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
%\usepackage[review]{acl}
\usepackage[]{acl}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsfonts}  % for mathbb
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0mm}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\usepackage{amsmath} % Required for align* environment


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript, and will typically save some space.
\usepackage{microtype}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in the typewriter font.
\usepackage{inconsolata}

\title{Deep dive into ANOVA(Analysis of Variance) Test}

\author{Van Nguyen \\
  Department of Computer Science \\
  Boise State University \\
  \texttt{vannguyen599@u.boisestate.edu}
}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}
Kickstarter is an online platform that enables creators to raise funds for their innovative projects from a community of supporters. The platform has become significantly popular among other crowdfunding platforms due to their user-friendly interface and dynamic ecosystem that fosters diverse creative campaigns. 
As someone who has backed several Kickstarter campaigns and found the experience satisfying, I at one point have considered launching some experimental project related to keyboards on the platform. 
Like other creators, the ultimate question is how to ensure their projects to be funded successfully.
Given this question, we need to engage in a study to understand what factors can influence the success of a project. 
The objective for this course project is to analyze all Kickstarter project data to identify patterns in order to predict successful funding campaign. 
The motivation is to employ data science techniques to analyze and uncover actionable insights that can inform future crowdfunding strategies.

\section{Methodology}
\label{sec:data_preprocessing}
The datasets used will be generated by \textit{https://webrobots.io/kickstarter-datasets/} which a platform where they have a regular scraper robot that crawls all Kickstarter projects every once a month.
we used the datasets through 2025-10-13, which have 186,372 data points and 46 columns
\subsection{Data Exploration}
Kickstarter does not provide any public API to retrieve data other then \textit{https://status.kickstarter.com/api/v2/summary.json} which is used to look up the status of the platform.
As a result, it is necessary to explore and investigate existing datasets from other platforms such as \textbf{Kaggle} to determine whether they are relevant or suitable to use. 
The selected dataset should be up-to-date and include all key factors required for an examination of funding trends and project success. 
we have attempted to scrape the data directly from the website but was not able to get all the sufficient data so we have decided to go with \textit{https://webrobots.io/kickstarter-datasets/}.
However to make sure that data is relevant and webrobots does not just patch missing data with wrong information, we have matched data we scraped with what currently provided by webrobots on 5-6 projects.
\textbf{sample.json} is the data that we have pulled down to check for webrobots's validity.

Other than that we need to go through the dataset in order to make sure that we only select our features for data training.
After consideration, we decided to go with the below list as features:
\begin{itemize}[itemsep=1mm]
  \item The campaign's financial goal in USD
  \item The time taken to prepare the campaign (from the creation date to the launch date)
  \item The duration of the campaign (from the launch date to the deadline)
  \item The geographical location of the campaign (whether it is based in the US or not)
  \item The category under which the campaign falls
\end{itemize}
We will need to do some data preprocessing where we dropped unused columns and check for duplicates or invalidate data points.
Other than that for any timestamp we want to convert them to datetime object. 
In order to accommodate the above feature list, we need to create extra columns that suit our purpose namely \textit{main\_category}, \textit{sub\_category}, \textit{campaign\_goal\_USD}, \textit{campaign\_location\_US}, \textit{campaign\_timeline}, \textit{prep\_timeline}.


\subsection{Data Analysis} 

We will use the preprocess dataset to analyze the trends evaluate how they affect the success of a campaign.
First we need to take a look at the success rate of all the campaigns in Figure~\ref{fig:success-rate}.
We can see that the success rate is roughly 60\% while the failed rate is around 40\% which is a not a significantly big gap.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/success-ratio.pdf} 
  \caption{Success Ratio}
\label{fig:success-rate} 
\end{figure}
Next, we take a look at the relationship between the campaign's financial goal and the success rate of it using box plot in Figure~\ref{fig:goal_usd}. 
Visually looking at the box plot graph, we can see that failed projects shows a massive number of outliers with very high goals up to \$140,000,000.
The primary takeaway from this plot is the significant difference in the magnitude and frequency of high-goal outliers between failed and successful projects.
Projects with extremely large goals are overwhelmingly found in the failed group. This strong concentration of high-goal outliers in the failed state suggests that setting a very high financial goal might be strongly correlated with project failure.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/goal_in_usd.pdf} 
  \caption{Goal in USD}
\label{fig:goal_usd} 
\end{figure}
Next, we take a look at the relationship between the campaign's geographical location and the success rate of it using Figure~\ref{fig:countries}, and Figure~\ref{fig:countries_success}.
By observation, the US accounts for the majority of launched campaigns on KickStarted, more than 60\% with over 66\% of those are successful projects.
The second country that accounts for the most projects launched is Great Britain, even though they are the second one but they only accounts for around 10\% with around 70\% of those are successful projects.
The gap between the number of launched products between US and Great Britain is significantly large.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/num_campaigns.pdf} 
  \caption{Campaign's country}
\label{fig:countries}  
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/success_campaigns_by_country.pdf} 
  \caption{Successful rate by country}
\label{fig:countries_success} 
\end{figure}


Next, we will observe the relationship between the campaign's duration and the rate of success of those projects in Figure~\ref{fig:cam_duration}.
The majority of campaigns fall within the first three duration bins (0-60 days), with very few campaigns lasting longer than 60 days.
The 20-40 day bin is overwhelmingly the most common campaign timeline, accounting for over 100,000 projects. 
Despite having a moderate success rate, the 20-40 day bin also has the highest absolute count of failed campaigns ($\approx 70,000$) due to the sheer volume of projects launched in this window.
The data suggests that a slightly longer 40-60 day timeline is associated with the highest likelihood of success
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/campaign_timeline_histogram.pdf} 
  \caption{Histogram of Campaign Timeline by Success}
\label{fig:cam_duration} 
\end{figure}
Other than that, looking at the preparation in time Figure~\ref{fig:prep_duration} we can see that the distribution is heavily skewed, with the vast majority of all projects falling into the first bin.
The shortest preparation timeline is by far the most popular choice for creators, reflecting a common behavior to launch soon after starting the project creation process.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/prep_timeline_histogram.pdf} 
  \caption{Histogram of Preparation Timeline by Success}
\label{fig:prep_duration} 
\end{figure}
Lastly, we will analyze the relationship between category and the rate of success Figure~\ref{fig:success_category}.
The chart demonstrates a clear concentration of successful campaigns in the Arts, Media, and Entertainment sectors, with Music and Film \& Video leading by a significant margin.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/success_campaigns_by_category.pdf} 
  \caption{Success Category}
\label{fig:success_category} 
\end{figure}

\section{Experiments}
In this section, we will be using the preprocessed data and then we will train them using 3 different model namely Logistic Regression, Decision Tree, and Random Forest to select the best model that yields that best performance.
All three models are typically evaluated using the same common classification performance metrics, which are designed to compare their predictions against the true labels.
We will initially train the models using their default parameters to evaluate their performance, and then proceed with hyperparameter tuning on the best-performing model.
The table~\ref{tab:training_summary} summarized all the training metrics after model training process.
The table shows that Logistic Regression and Random Forest have only slight differences across all evaluated metrics. 
However, Random Forest performs slightly better in both Accuracy and ROC.
\begin{table}[h]
\centering
\begin{tabular}{p{1cm}ccccc}
\toprule
{\bf Model type} & {\bf accuracy} & {\bf precision} & {\bf recall} & {\bf f1} & {\bf roc} \\
\midrule
 Logistic & 0.690  & 0.701& 0.866& 0.774 & 0.712\\
 Decision Tree & 0.643 & 0.717  & 0.693 & 0.705 & 0.631 \\
 Random Forest & 0.695  & 0.734 & 0.790 & 0.761 & 0.730\\
\bottomrule
\end{tabular}
\caption{Summary of chosen model's performance before tuning}
\label{tab:training_summary}
\end{table}
In order to see it better, we also have a ROC curve of all the three models in Figure~\ref{fig:roc_curve}. 
From the graph we can see that Random Forest model has the highest AUC at $0.73$4, indicating it is the overall best classifier among the three, as it has the best ability to discriminate between the positive and negative classes
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/roc_curves.pdf} 
  \caption{ROC Curves}
\label{fig:roc_curve} 
\end{figure}

\subsection{Hyperparam tuning}
We opted to fine-tune the Random Forest parameters using GridSearchCV due to its superior overall performance.
Our goal was to reduce false positives and enhance precision.
After param tuning, the best parameter that we got is criterion: entrophy and n\_estimator: 500 which yields the following improvement in table~\ref{tab:after_train_summary} 

\begin{table}[h]
\centering
\begin{tabular}{ccccc}
\toprule
{\bf accuracy} & {\bf precision} & {\bf recall} & {\bf f1} & {\bf roc} \\
\midrule
 0.7007 & 0.7379  &  0.7968 & 0.76626 & 0.735\\
\bottomrule
\end{tabular}
\caption{Summary of chosen model's performance of Random Forest after tuning}
\label{tab:after_train_summary}
\end{table}

Commonly used All of the steps used in this paper used the guidance from ~\cite{datacamp_anova_tutorial} post-hoc tests include Tukey’s Honestly Significant Difference (HSD) ~\cite{howell2010statistical} and the Bonferroni Correction ~\cite{datacamp_anova_tutorial}.
Commonly used post-hoc tests include Tukey’s Honestly Significant Difference (HSD) ~\cite{armstrong2014bonferroni} and the Bonferroni Correction ~\cite{datacamp_anova_tutorial}.


\section{Conclusion}

In this paper, we explored the fundamental concepts and applications of the Analysis of Variance (ANOVA) test especially one-way ANOVA. 
ANOVA is a powerful statistical tool used to determine whether there are significant differences between the means of multiple groups.
We discussed the core concepts, including getting all the assumptions required for valid results, and interpreted the final result( including whether or not the null hypothesis is accepted).
Additionally, we emphasized the importance of post-hoc tests, such as Tukey’s HSD and the Bonferroni Correction, in identifying specific group differences when the ANOVA results are significant.
Overall, ANOVA remains an essential technique in experimental design and data analysis, providing researchers with a robust method for comparing group means and uncovering insights in diverse fields such as education, agriculture, and healthcare. 
By understanding its principles and limitations, practitioners can effectively apply ANOVA to real-world problems and make data-driven decisions.

% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
\bibliographystyle{acl_natbib}

\end{document}